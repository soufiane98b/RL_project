{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "from flask import Flask, render_template\n",
    "from sklearn.datasets import make_blobs \n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random\n",
    "from itertools import groupby\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from functools import reduce\n",
    "from itertools import product\n",
    "from itertools import combinations\n",
    "import scipy.stats as stats\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "import linucb\n",
    "import json\n",
    "from kmodes.kmodes import KModes\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choix_compo_features(nb_feature,nb_min_categorie,nb_max_categorie):\n",
    "    \n",
    "    dict_composition_features = dict() # contient le nombre de catégorie pour chaque feature \n",
    "    columns = []\n",
    "    nombre_individu_distinct = 1\n",
    "    for i in range(nb_feature):\n",
    "        feature = 'feature'+str(i)\n",
    "        dict_composition_features[feature] = random.randint(nb_min_categorie, nb_max_categorie)\n",
    "        columns.append(feature)\n",
    "        nombre_individu_distinct = nombre_individu_distinct * dict_composition_features[feature]\n",
    "\n",
    "    return dict_composition_features , nombre_individu_distinct\n",
    "\n",
    "def generer_des_groupes(dict_composition_features,nb_groupes):\n",
    "    groupes = []\n",
    "    for i in range(nb_groupes):\n",
    "        groupe_i = []\n",
    "        for feature in dict_composition_features:\n",
    "            max = dict_composition_features[feature]\n",
    "            nb = random.randint(1,max)\n",
    "            groupe_i.append(random.sample(range(1,max+1), nb))\n",
    "        groupes.append(groupe_i)\n",
    "    return groupes\n",
    "\n",
    "def nb_possibilite(groupes):\n",
    "    return [reduce((lambda x, y: x * y), list(map((lambda x: len(x) ), g))) for g in groupes]\n",
    "\n",
    "def distance(x,y):\n",
    "    set1 = set(x)\n",
    "    set2 = set(y)\n",
    "    similarity = len(set1.intersection(set2)) / len(set1.union(set2))\n",
    "    return 1 - similarity\n",
    "\n",
    "def distance_groupes(x,y):\n",
    "    l = [distance(x[i],y[i]) for i in range(len(x)) ]\n",
    "    return sum(l) / len(l)\n",
    "\n",
    "def distance_moyenne_entre_paires_groupes(groupes):\n",
    "    cmpt = 0\n",
    "    summ = 0\n",
    "    list_dist = []\n",
    "    for pair in itertools.combinations(list(range(len(groupes))), 2):\n",
    "        cmpt = cmpt + 1\n",
    "        tmp = distance_groupes(groupes[pair[0]],groupes[pair[1]])\n",
    "        summ = summ + tmp\n",
    "        list_dist.append((pair[0],pair[1],tmp))\n",
    "    return summ/cmpt , list_dist\n",
    "\n",
    "\n",
    "def appartenance_groupe(individu,groupes):\n",
    "    ind=0\n",
    "    indices = []\n",
    "    for g in groupes:\n",
    "        test = True\n",
    "        for i in range(len(g)):\n",
    "            if individu[i] not in g[i]:\n",
    "                test = False\n",
    "                break\n",
    "        if test == True:\n",
    "            indices.append(ind)\n",
    "        ind=ind+1\n",
    "    return indices\n",
    "\n",
    "\n",
    "def nombre_individu_par_groupes(df,groupes):\n",
    "    cmpt_gr = [0] * len(groupes)\n",
    "    intersection_cmpt = [0] * len(groupes)\n",
    "    sans = 0\n",
    "    for i in range(len(df)):\n",
    "        individu = list(df.iloc[i])\n",
    "        L = appartenance_groupe(individu,groupes)\n",
    "        if L == [] :\n",
    "            sans = sans + 1\n",
    "        else :\n",
    "            intersection_cmpt[len(L)-1] = intersection_cmpt[len(L)-1] + 1\n",
    "        for l in L :\n",
    "            cmpt_gr[l] = cmpt_gr[l] + 1\n",
    "    return cmpt_gr , intersection_cmpt, sans\n",
    "\n",
    "def predict_nombre_individu_par_groupes_sum(taille_df,nombre_individu_distinct,nb_possibilite):\n",
    "    tmp = nb_possibilite / nombre_individu_distinct\n",
    "    return tmp * taille_df\n",
    "\n",
    "\n",
    "def choisir_alea(groupe,nb):\n",
    "    max_unique_combi = nb_possibilite([groupe])[0]\n",
    "    num_combos = min (max_unique_combi , nb)\n",
    "    combos = []\n",
    "    while len(combos) < num_combos :\n",
    "        combo = []\n",
    "        for g in groupe :\n",
    "            combo.append(random.sample(g, 1)[0])\n",
    "        if combo not in combos :\n",
    "            combos.append(combo)\n",
    "    return combos\n",
    "\n",
    "\n",
    "def generer_all_unique_possi_df(groupe, id_groupe,taille):\n",
    "    columns = ['feature'+str(f) for f in range(len(groupe))]\n",
    "    combinations  = choisir_alea(groupe,taille)\n",
    "    df = pd.DataFrame(combinations,columns=columns)\n",
    "    df = df.assign(id_groupe = id_groupe)\n",
    "    return df\n",
    "\n",
    "def generer_all_unique_possi_df_for_groups(groupes,list_id,taille) :\n",
    "    columns = ['feature'+str(f) for f in range(len(groupes[0]))]\n",
    "    columns.append('id_groupe')\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    for i in list_id :\n",
    "        tmp_df = generer_all_unique_possi_df(groupes[i], i,taille)\n",
    "        df = pd.concat([df, tmp_df], axis=0)\n",
    "    return df.reset_index().drop('index', axis=1)\n",
    "\n",
    "def reblance_df(df,nb_groupes):\n",
    "    # Et Avoir aussi au moins 10 rows, ca sera notre base pour entrainer le modèle \n",
    "    max_individu_grp = max(df['id_groupe'].value_counts())\n",
    "    for id in range(nb_groupes) :\n",
    "        nb_individu = (df['id_groupe']==id).sum()\n",
    "        if(nb_individu<max_individu_grp):\n",
    "            duppli =(max_individu_grp // nb_individu) - 1\n",
    "            duppli_rand = max_individu_grp % nb_individu\n",
    "            tmp_df = df[df['id_groupe']==id]\n",
    "            if duppli > 0 :\n",
    "                tmp_df = pd.concat([tmp_df] * duppli, ignore_index=True)\n",
    "                tmp_df = pd.concat([tmp_df,tmp_df.sample(n=duppli_rand)], ignore_index=True)\n",
    "            if duppli == 0:\n",
    "                tmp_df = tmp_df.sample(n=duppli_rand)\n",
    "            df = pd.concat([df,tmp_df], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def shape_train_df(df,nb_rows,len_group,ratio_balanced,dist='uniform'):\n",
    "    distributions = {\"expon\": stats.expon(loc=0, scale=1),\"poisson\": stats.poisson(mu=1),\"gamma\": stats.gamma(a=1, loc=0, scale=1),\"pareto\": stats.pareto(b=1),\"lognorm\": stats.lognorm(s=1, loc=0, scale=1),\"uniform\": stats.uniform(loc=0, scale=1)}\n",
    "    # Le ratio commun entre 0 et 1 est départagé equitablement entre les différents groupes puis le 1 - ratio_commun est departagé inequitablement pour le desequilibre\n",
    "    if (ratio_balanced<0 or ratio_balanced>1 ):\n",
    "        print(\"Ratio not between 0 and 1\")\n",
    "        return None \n",
    "    base_ratios = [ratio_balanced/len_group for i in range(len_group)]\n",
    "    unblanced_ratio = 1 - ratio_balanced\n",
    "    dist_ratios = distributions[dist].rvs(size=len_group)\n",
    "    dist_ratios = dist_ratios / sum(dist_ratios)\n",
    "    dist_ratios = sorted(dist_ratios, reverse=True)\n",
    "    dist_ratios = [r * unblanced_ratio for r in dist_ratios]\n",
    "    wanted_groups_number = [int((x + y)*nb_rows) for x, y in zip(base_ratios, dist_ratios)] \n",
    "    random.shuffle(wanted_groups_number) \n",
    "    actual_groups_number = df['id_groupe'].value_counts()[0] \n",
    "\n",
    "    final_df = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "    for id_g  in range(len_group):\n",
    "        wanted = wanted_groups_number[id_g]\n",
    "        tmp_df = df[df['id_groupe']==id_g]\n",
    "        if(wanted <= actual_groups_number):\n",
    "            final_df = pd.concat([final_df,tmp_df.sample(n=wanted)],ignore_index=True)\n",
    "        else :\n",
    "            duppli =(wanted // actual_groups_number)\n",
    "            duppli_rand = wanted % actual_groups_number\n",
    "            tmp_df = pd.concat([tmp_df] * duppli, ignore_index=True)\n",
    "            tmp_df = pd.concat([tmp_df,tmp_df.sample(n=duppli_rand)], ignore_index=True)\n",
    "            final_df = pd.concat([final_df,tmp_df], ignore_index=True)\n",
    "    \n",
    "    return final_df \n",
    "\n",
    "\n",
    "def compter_diversite(df):\n",
    "    num_groupes = list(df['id_groupe'].unique())\n",
    "    disp = []\n",
    "    for n in num_groupes :\n",
    "        tmp_df = df[df['id_groupe']==n]\n",
    "        tmp_df = tmp_df[tmp_df.columns[:-1]]\n",
    "        prop = tmp_df.duplicated().sum() * 100 / len(tmp_df)\n",
    "        disp.append((n,str(int(prop))+'% duplique',len(tmp_df) ) )\n",
    "    return disp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_dictionnaire_data(data):\n",
    "    f=pd.DataFrame(data['other_info']['composition des features'],index=['nb of class'])\n",
    "    ##################################################################################################\n",
    "    ind = ['compo groupe '+str(i) for i in range(len(data['other_info']['composition des groupes']))]\n",
    "    df = pd.DataFrame(data['other_info']['composition des groupes'],columns=f.columns,index =ind )\n",
    "    ##################################################################################################\n",
    "    general = pd.concat([f,df])\n",
    "    general['nb disctint person possible'] = ['-']+data['other_info']['nombre possibilite individu dans les groupes']\n",
    "    general['diversite des groupes'] = ['-']+data['other_info']['diversite des groupes']\n",
    "    data['info_df'] = general\n",
    "    tmp = data['other_info']\n",
    "    del data['other_info']\n",
    "    del tmp['composition des features']\n",
    "    del tmp['composition des groupes']\n",
    "    del tmp['nombre possibilite individu dans les groupes']\n",
    "    del tmp['diversite des groupes'] \n",
    "    data['Other Information']= tmp\n",
    "    return data\n",
    "\n",
    "def ordonner(Datas):\n",
    "    return sorted(Datas, key=lambda x: int(x[1][-2:])) # ATTENTION A CHANGER CAR MARCHE SI GROUPES SUR 2 CHIFFRE SINON ERREUR !!!!!\n",
    "\n",
    "def generete_data(nombre_feature,nombre_div_cat_min,nombre_div_cat_max,nombre_groupe,nb_lignes,ratio_de_disparite_inter_group,dist):\n",
    "    dict_info_data = {}\n",
    "    ############################\n",
    "    dict_info_data['nombre de ligne']=nb_lignes\n",
    "    dict_info_data['ratio_de_disparite_inter_group'] = ratio_de_disparite_inter_group\n",
    "    dict_info_data['Loi de distribution de dispartibe'] = dist\n",
    "    ############################\n",
    "    dict_composition_features , nombre_individu_distinct = choix_compo_features(nombre_feature,nombre_div_cat_min,nombre_div_cat_max)\n",
    "    dict_info_data['nb de col avec one hot'] = sum(dict_composition_features.values())\n",
    "    dict_info_data['indiviud distinct'] = nombre_individu_distinct\n",
    "    dict_info_data['composition des features'] = dict_composition_features\n",
    "    #print(nombre_individu_distinct)\n",
    "    #print(sum(dict_composition_features.values()))\n",
    "    ############################\n",
    "    groupes = generer_des_groupes(dict_composition_features,nombre_groupe)\n",
    "    dict_info_data['composition des groupes'] = groupes\n",
    "    moy , list_dist = distance_moyenne_entre_paires_groupes(groupes)\n",
    "    nb_possi = nb_possibilite(groupes)\n",
    "    dict_info_data['distance moyenne entre paires groupes'] = moy\n",
    "    dict_info_data['nombre possibilite individu dans les groupes'] = nb_possi\n",
    "    dict_info_data['distance entre paires groupes '] = list_dist\n",
    "    ############################\n",
    "    test = True\n",
    "    while test :\n",
    "        df = generer_all_unique_possi_df_for_groups(groupes,list(range(len(groupes))),20000) # ->>>>> A VARIER SI BESOIN SI BCP BCP DE LIGNES\n",
    "        df = df.sample(frac=1).reset_index(drop=True)\n",
    "        df.drop_duplicates(subset=list(df.columns[:-1]), keep='last', inplace=True) # <----------- Enleve deux individu pareils pour ne pas etre dans 2 groupes differents\n",
    "        # Car donne de mauvais resultats pour les algos voir si cas possible dans la realite , se produit quand très peu de nb individu discints \n",
    "        if len(df['id_groupe'].unique()) == len(groupes) : # ca veut dire un groupe inclu dans un autre et on veut pas ca , si egal ok\n",
    "            test = False\n",
    "            # Alors ok on peut passer étape suivante\n",
    "    ############################\n",
    "    df = reblance_df(df,len(groupes))\n",
    "    final_df = shape_train_df(df,nb_lignes,len(groupes),ratio_de_disparite_inter_group,dist)\n",
    "    dict_info_data['diversite des groupes'] = compter_diversite(final_df)\n",
    "    ############################\n",
    "    cmpt_gr , intersection_cmpt, sans= nombre_individu_par_groupes(final_df,groupes)\n",
    "    dict_info_data['nb individu ds chaque groupes'] = cmpt_gr\n",
    "    dict_info_data['nb de cross'] = intersection_cmpt\n",
    "    ############################\n",
    "    df_encoded = pd.get_dummies(final_df,columns=list(final_df.columns)[:-1]).astype(int)\n",
    "    df_encoded['cluster']= df_encoded['id_groupe']\n",
    "    df_encoded=df_encoded.drop('id_groupe',axis=1)\n",
    "    df_encoded = df_encoded.sample(frac=1).reset_index(drop=True)\n",
    "    dict_info_data['nombre d individu duplique en % dans final df'] =(df_encoded.duplicated().sum())*100/len(df_encoded)\n",
    "        \n",
    "    if  (df_encoded.shape[1] != sum(dict_composition_features.values()) + 1):\n",
    "        print('Il y aura probleme de noramlisation dans méthode 2 dans dict_composition_features')\n",
    "\n",
    "    return final_df , df_encoded , dict_info_data\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"nb_de_colonne = [5,10,15]\\nnb_categorie_feature =[5,12]\\nnombre_groupe = [10,30,50] \\navancement = len(nb_de_colonne)*len(nb_categorie_feature)*len(nombre_groupe)\\nchemin = '/Users/soufiane/Documents/GitHub/universe/My_universe/Data' # ATTENTION A CONFIGURER SELON ENVIRONEMENT\\n\\ni=1\\nfor c in nb_de_colonne:\\n    for f in nb_categorie_feature :\\n        for g in nombre_groupe:\\n            final_df , df_encoded , dict_info_data = generete_data(c,2,f,g,30000,1,'uniform')\\n            fichier_name = chemin+'DataSimuRl_c_'+str(c)+'_f_'+str(f)+'_g_'+str(g)\\n            data = {'nb_de_colonne':c,'nb_categorie_max_par_colonne':f,'nombre_groupe':g,'df': final_df, 'df_encoded':df_encoded,'other_info': dict_info_data}\\n            with open(fichier_name, 'wb') as fichier:\\n                pickle.dump(data, fichier)\\n            print(fichier_name,' | DONE ',' | Avancement : ',i,' sur ',avancement)\\n            i=i+1\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_de_colonne = [5,10,15]\n",
    "nb_categorie_feature =[5,12]\n",
    "nombre_groupe = [10,30,50] \n",
    "avancement = len(nb_de_colonne)*len(nb_categorie_feature)*len(nombre_groupe)\n",
    "chemin = '/Users/soufiane/Documents/GitHub/universe/My_universe/Data' # ATTENTION A CONFIGURER SELON ENVIRONEMENT\n",
    "\n",
    "i=1\n",
    "for c in nb_de_colonne:\n",
    "    for f in nb_categorie_feature :\n",
    "        for g in nombre_groupe:\n",
    "            final_df , df_encoded , dict_info_data = generete_data(c,2,f,g,30000,1,'uniform')\n",
    "            fichier_name = chemin+'DataSimuRl_c_'+str(c)+'_f_'+str(f)+'_g_'+str(g)\n",
    "            data = {'nb_de_colonne':c,'nb_categorie_max_par_colonne':f,'nombre_groupe':g,'df': final_df, 'df_encoded':df_encoded,'other_info': dict_info_data}\n",
    "            with open(fichier_name, 'wb') as fichier:\n",
    "                pickle.dump(data, fichier)\n",
    "            print(fichier_name,' | DONE ',' | Avancement : ',i,' sur ',avancement)\n",
    "            i=i+1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_reward(cluster_id,arm_id,nb_arm):\n",
    "    penality = 0\n",
    "    if cluster_id%nb_arm == arm_id: # in this case we choosed the good arm but doesn't mean for sure postive reward but high proba\n",
    "        p = 0.8 #+ random.uniform(-0.1, 0.2)\n",
    "        return random.choices([penality,1], [1-p,p])[0]\n",
    "    else :\n",
    "        p = 0.1 #+ random.uniform(-0.05, 0.05)\n",
    "        return random.choices([penality,1], [1-p,p])[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_reward_shift(cluster_id,arm_id,nb_gps,nb_arm):\n",
    "    prefered_arm =  shift(cluster_id,nb_gps)\n",
    "    penality = 0\n",
    "    if prefered_arm%nb_arm == arm_id: # in this case we choosed the good arm but doesn't mean for sure postive reward but high proba\n",
    "        p = 0.6 + random.uniform(-0.1, 0.2)\n",
    "        return random.choices([penality,1], [1-p,p])[0]\n",
    "    else :\n",
    "        p = 0.1 + random.uniform(-0.05, 0.05)\n",
    "        return random.choices([penality,1], [1-p,p])[0]\n",
    "    \n",
    "\n",
    "def shift(c_id,nb_gps):\n",
    "    t = (nb_gps-1) - c_id\n",
    "    if (t<0 or t>nb_gps-1):\n",
    "        print(\"nb_gps-1 : \",nb_gps-1,\"c_id\",c_id,\" t:\",t)\n",
    "        print('erreur dans shift')\n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_ucb_ev_dict(listt):\n",
    "    result = {}\n",
    "    for i in listt :\n",
    "        result[i]={'p':[],'theta':[],'Incert':[]}\n",
    "    return result\n",
    "\n",
    "\n",
    "def evolution_paramètre_arm(arm_id,listt,linucb_policy_object):\n",
    "    arm_ucb_ev=linucb_policy_object.linucb_arms[arm_id].ucb_evo\n",
    "    result=initiate_ucb_ev_dict( listt)\n",
    "    for e in arm_ucb_ev:\n",
    "        feature = e[0]\n",
    "        result[feature]['p'].append(float(e[1]))\n",
    "        result[feature]['theta'].append(float(e[2]))\n",
    "        result[feature]['Incert'].append(float(e[3]))\n",
    "    return result\n",
    "\n",
    "# FOR EVOLUTION OF ARM : THETA , P and INCERTITUDE OF POLICY\n",
    "\n",
    "def Evolution_Arm(arm,df_encoded,linucb_policy_object,fromm=0,to=-1):\n",
    "    arm = 0\n",
    "    fromm = 0\n",
    "    to = -1\n",
    "    listt = list(df_encoded['cluster'].value_counts().index)\n",
    "    result=evolution_paramètre_arm(arm,listt,linucb_policy_object)\n",
    "    for i in listt:\n",
    "        plt.plot(result[i]['Incert'][fromm:to], label='theta of group'+str(i))\n",
    "    plt.title(\"Evolution du Incert pour chaque feature de l'arm 0 \")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    for i in listt:\n",
    "        plt.plot(result[i]['theta'][fromm:to], label='theta of group'+str(i))\n",
    "    plt.title(\"Evolution du theta pour chaque feature de l'arm 0 \")\n",
    "    #plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    for i in listt:\n",
    "        plt.plot(result[i]['p'][fromm:to], label='theta of group'+str(i))\n",
    "    plt.title(\"Evolution du p pour chaque feature de l'arm 0 \")\n",
    "    #plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# FOR EVOLUTION OF FEATURES ARM :  THETA , A and B FOR A POLICY\n",
    "\n",
    "def Evoluion_feature_arm(arm,linucb_policy_object,fromm=0,to=-1):\n",
    "    # Créer l'array de données\n",
    "    arm = 0\n",
    "    fromm = 0\n",
    "    to = -1\n",
    "    axe=0\n",
    "    dataT = np.array(linucb_policy_object.linucb_arms[arm].theta_list[fromm:to])\n",
    "    dataB = np.array(linucb_policy_object.linucb_arms[arm].b_list[fromm:to])\n",
    "    dataA = np.array(linucb_policy_object.linucb_arms[arm].A_theta_list[fromm:to])\n",
    "\n",
    "    # Afficher l'évolution de chaque point sur une courbe\n",
    "    for i in range(dataT.shape[1]):\n",
    "        plt.plot(dataT[:,i,:], label='feature {}'.format(i))\n",
    "\n",
    "    plt.legend()\n",
    "    plt.axvline(x=axe, color='black')\n",
    "    plt.title(\"EVOLUTION DU THETA\")\n",
    "    plt.show()\n",
    "\n",
    "    # Afficher l'évolution de chaque point sur une courbe\n",
    "    for i in range(dataB.shape[1]):\n",
    "        plt.plot(dataB[:,i,:], label='feature {}'.format(i))\n",
    "    #----\n",
    "    #plt.legend()\n",
    "    #plt.axvline(x=axe, color='black')\n",
    "    plt.title(\"EVOLUTION DU B\")\n",
    "    plt.show()\n",
    "\n",
    "    # Afficher l'évolution de chaque point sur une courbe\n",
    "    for i in range(dataA.shape[1]):\n",
    "        plt.plot(dataA[:,i,:], label='feature {}'.format(i))\n",
    "    #----\n",
    "    #plt.legend()\n",
    "    #plt.axvline(x=axe, color='black')\n",
    "    plt.title(\"EVOLUTION DU A\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctr_simulator_policies(df,df_clustered,nb_groups,policies,shift):\n",
    "    # Instantiate trackers\n",
    "    cumulative_rewards_b_a = []\n",
    "    n_policies = len(policies)\n",
    "    aligned_time_steps = np.zeros(n_policies)\n",
    "    cumulative_rewards = np.zeros(n_policies)\n",
    "    aligned_ctr = [[] for i in range(n_policies)]\n",
    "    aligned_time = np.zeros(n_policies)\n",
    "    # For updating arm indexes\n",
    "    arm_index = np.zeros(n_policies)\n",
    "    # For updating arm rewards\n",
    "    data_reward = np.zeros(n_policies)\n",
    "    \n",
    "    # BEFORE SHIFT\n",
    "    #print(\"Debut sans shift\")\n",
    "    for i in range(len(df)):\n",
    "\n",
    "        # Selecting arm index et simulation reward and updating for each policies \n",
    "        for p in range(len(policies)):\n",
    "            # We calculate the time for each iteration  \n",
    "            start_time = time.time()\n",
    "            ##########\n",
    "            if p==2:\n",
    "                # Recupere Data\n",
    "                array = np.array(df.iloc[i])\n",
    "                data_x_array = np.delete(array,-1) # enlève dernier élement \n",
    "                ######################\n",
    "                arm_index[p] = random.randint(0, 9) # WE HAVE 10 ARMS \n",
    "                data_reward[p] = simulate_reward(array[-1],arm_index[p],10)\n",
    "            else :\n",
    "                if p == 3:\n",
    "                    # Recupere Data\n",
    "                    array = np.array(df_clustered.iloc[i])\n",
    "                    data_x_array = np.delete(array,-1) # enlève dernier élement \n",
    "                else :\n",
    "                    # Recupere Data\n",
    "                    array = np.array(df.iloc[i])\n",
    "                    data_x_array = np.delete(array,-1) # enlève dernier élement \n",
    "\n",
    "                #print(' p  : ',p,' data : ',data_x_array)\n",
    "                arm_index[p] = policies[p].select_arm(data_x_array)\n",
    "                data_reward[p] = simulate_reward(array[-1],arm_index[p],policies[p].K_arms)\n",
    "                # Use reward information for the chosen arm to update\n",
    "                tmp_arm = int(arm_index[p])\n",
    "                policies[p].linucb_arms[tmp_arm].reward_update(data_reward[p], data_x_array)\n",
    "                ##########\n",
    "            end_time = time.time()\n",
    "            elapsed_time = end_time - start_time\n",
    "            aligned_time[p]  = aligned_time[p] + elapsed_time\n",
    "\n",
    "            # For CTR calculation\n",
    "            aligned_time_steps[p] +=1\n",
    "            cumulative_rewards[p] += data_reward[p]\n",
    "            aligned_ctr[p].append(cumulative_rewards[p]/aligned_time_steps[p])\n",
    "        #if (i%1000==0):\n",
    "            #print(str(i*100/len(df))+'%')\n",
    "    #print(\"Fin sans shift \",cumulative_rewards)\n",
    "    cumulative_rewards_b_a.append(cumulative_rewards)\n",
    "    ################################################################################################################################################\n",
    "    #print('Debut du Shift')\n",
    "    cumulative_rewards = np.zeros(n_policies)\n",
    "    aligned_time_steps = np.zeros(n_policies)\n",
    "    # After SHIFT\n",
    "    #print(\"Debut avec shift\")\n",
    "    for i in range(shift):\n",
    "        \n",
    "        # Selecting arm index et simulation reward and updating for each policies \n",
    "        for p in range(len(policies)):\n",
    "            # We calculate the time for each iteration  \n",
    "            start_time = time.time()\n",
    "            ##########\n",
    "            if p==2:\n",
    "                # Recupere Data\n",
    "                array = np.array(df.iloc[i])\n",
    "                data_x_array = np.delete(array,-1) # enlève dernier élement \n",
    "                ######################\n",
    "                arm_index[p] = random.randint(0, 9) # WE HAVE 10 ARMS \n",
    "                data_reward[p] = simulate_reward_shift(array[-1],arm_index[p],nb_groups,10)# WE HAVE 10 ARMS \n",
    "            else :\n",
    "                if p == 3:\n",
    "                    # Recupere Data\n",
    "                    array = np.array(df_clustered.iloc[i])\n",
    "                    data_x_array = np.delete(array,-1) # enlève dernier élement \n",
    "                else :\n",
    "                    # Recupere Data\n",
    "                    array = np.array(df.iloc[i])\n",
    "                    data_x_array = np.delete(array,-1) # enlève dernier élement \n",
    "\n",
    "                arm_index[p] = policies[p].select_arm(data_x_array)\n",
    "                data_reward[p] = simulate_reward_shift(array[-1],arm_index[p],nb_groups,policies[p].K_arms)\n",
    "                # Use reward information for the chosen arm to update \n",
    "                tmp_arm = int(arm_index[p])\n",
    "                policies[p].linucb_arms[tmp_arm].reward_update(data_reward[p], data_x_array)\n",
    "            ##########\n",
    "            end_time = time.time()\n",
    "            elapsed_time = end_time - start_time\n",
    "            aligned_time[p]  = aligned_time[p] + elapsed_time\n",
    "\n",
    "            # For CTR calculation\n",
    "            aligned_time_steps[p] +=1\n",
    "            cumulative_rewards[p] += data_reward[p]\n",
    "            aligned_ctr[p].append(cumulative_rewards[p]/aligned_time_steps[p])\n",
    "        #if (i%1000==0):\n",
    "            #print(str(i*100/shift)+'%')\n",
    "    #print(\"Fin avec shift \",cumulative_rewards)\n",
    "    cumulative_rewards_b_a.append(cumulative_rewards)\n",
    "\n",
    "    return (cumulative_rewards_b_a, aligned_ctr ,aligned_time,policies)\n",
    "\n",
    "\n",
    "def RunSimuOnData(Data,l,l_shift):\n",
    "    df_encoded = Data['df_encoded']\n",
    "    nb_groups = Data['nombre_groupe']\n",
    "    df_clustered_encoded = Data['data_clustered_encoded'].sample(frac=1, axis=1)\n",
    "    if l != -1 : # Sinon on parcout tout le data frame\n",
    "        df_encoded = df_encoded.head(l)\n",
    "    dict_info_data = dict(Data['info_df'].loc['nb of class'])\n",
    "    del dict_info_data['nb disctint person possible']\n",
    "    del dict_info_data['diversite des groupes']\n",
    "\n",
    "    # Choice of policies \n",
    "    L_UCB = linucb.linucb_policy(K_arms = 10, d = df_encoded.shape[1]-1, alpha=1,version= -1,df_encoded=df_encoded)\n",
    "    L_UCB_BIS = linucb.linucb_policy_bis(K_arms = 10, d = df_encoded.shape[1]-1, alpha=0.5,version= 1,compo_feature=dict_info_data,df_encoded=df_encoded)\n",
    "    L_UCB_BIS_clustered = linucb.linucb_policy_bis(K_arms = 10, d = df_clustered_encoded.shape[1]-1, alpha=0.5,version= 1,compo_feature=-1,df_encoded=df_clustered_encoded)\n",
    "    policies = [L_UCB,L_UCB_BIS,'Random',L_UCB_BIS_clustered]\n",
    "\n",
    "    # Run the Simulation and recover data of simu\n",
    "    cumulative_rewards, aligned_ctr ,aligned_time ,policies = ctr_simulator_policies(df_encoded,df_clustered_encoded,nb_groups,policies,l_shift)\n",
    "    \n",
    "    return cumulative_rewards, aligned_ctr ,aligned_time ,policies\n",
    "\n",
    "\n",
    "def export_simu_info(cumulative_rewards, aligned_ctr ,aligned_time ,policies):\n",
    "    info_simu = dict()\n",
    "    ########\n",
    "    info_simu['Aligned CTR'] = [(aligned_ctr[0],'Linear UCB'),(aligned_ctr[1],'Linear UCB BIS'),(aligned_ctr[2],'Random'),(aligned_ctr[3],'BIS Clustering')]\n",
    "    ########\n",
    "    df = pd.DataFrame(aligned_time,index=['Linear UCB Disjoint','Linear UCB Disjoint BIS','Random','BIS Clustering'],columns=[\"Execution time\"]).T\n",
    "    tmp = pd.DataFrame(cumulative_rewards[0],index=['Linear UCB Disjoint','Linear UCB Disjoint BIS','Random','BIS Clustering'],columns=[\"Cumulative Rewards Before Shift\"]).T\n",
    "    df = pd.concat([df , tmp])\n",
    "    tmp = pd.DataFrame(cumulative_rewards[1],index=['Linear UCB Disjoint','Linear UCB Disjoint BIS','Random','BIS Clustering'],columns=[\"Cumulative Rewards After Shift\"]).T\n",
    "    df = pd.concat([df , tmp])\n",
    "    tmp = pd.DataFrame(sum(cumulative_rewards),index=['Linear UCB Disjoint','Linear UCB Disjoint BIS','Random','BIS Clustering'],columns=[\"Cumulative Rewards Total\"]).T\n",
    "    df = pd.concat([df , tmp])\n",
    "    info_simu['Information sur Reward et execution'] = df\n",
    "    return info_simu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordonner(Datas):\n",
    "    return sorted(Datas, key=lambda x: int(x[1].split('_')[2])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chemin = '/Users/soufiane/Documents/GitHub/universe/My_Universe/Data_with_clustering'\n",
    "fichiers = os.listdir(chemin)\n",
    "Datas = []\n",
    "for nom_fichier in fichiers:\n",
    "    with open(chemin+'/'+nom_fichier, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        #data = reshape_dictionnaire_data(data) , not use for Data_with_clustering , already reshaped\n",
    "        Datas.append((data,nom_fichier))\n",
    "Datas = ordonner(Datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debut de Simulation pour fichier : DataSimuRl_c_5_f_5_g_30\n",
      "Done now we dump\n",
      "Fin de Simulation pour fichier : DataSimuRl_c_5_f_5_g_30  | Avancement :  1  sur  18\n",
      "------------------------------------------------------------------------------------\n",
      "Debut de Simulation pour fichier : DataSimuRl_c_5_f_12_g_30\n",
      "Done now we dump\n",
      "Fin de Simulation pour fichier : DataSimuRl_c_5_f_12_g_30  | Avancement :  2  sur  18\n",
      "------------------------------------------------------------------------------------\n",
      "Debut de Simulation pour fichier : DataSimuRl_c_5_f_5_g_50\n",
      "Done now we dump\n",
      "Fin de Simulation pour fichier : DataSimuRl_c_5_f_5_g_50  | Avancement :  3  sur  18\n",
      "------------------------------------------------------------------------------------\n",
      "Debut de Simulation pour fichier : DataSimuRl_c_5_f_12_g_50\n",
      "Done now we dump\n",
      "Fin de Simulation pour fichier : DataSimuRl_c_5_f_12_g_50  | Avancement :  4  sur  18\n",
      "------------------------------------------------------------------------------------\n",
      "Debut de Simulation pour fichier : DataSimuRl_c_5_f_12_g_10\n",
      "Done now we dump\n",
      "Fin de Simulation pour fichier : DataSimuRl_c_5_f_12_g_10  | Avancement :  5  sur  18\n",
      "------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "chemin = '/Users/soufiane/Documents/GitHub/universe/My_universe/TT/'\n",
    "i=1\n",
    "for d in Datas[:5]:\n",
    "    print('Debut de Simulation pour fichier : '+d[1])\n",
    "    cumulative_rewards, aligned_ctr ,aligned_time ,policies = RunSimuOnData(d[0],-1,15000)\n",
    "    info_simu = export_simu_info(cumulative_rewards, aligned_ctr ,aligned_time ,policies)\n",
    "    d[0]['Inforamtion sur la Simulation'] = info_simu\n",
    "    # On enregistre les données dans le répertoire\n",
    "    print('Done now we dump')\n",
    "    fichier_name = chemin + d[1]\n",
    "    with open(fichier_name, 'wb') as fichier:\n",
    "        try:\n",
    "            pickle.dump(d[0], fichier)\n",
    "            print('Fin de Simulation pour fichier : '+d[1],' | Avancement : ',i,' sur ',18)\n",
    "            print('------------------------------------------------------------------------------------')\n",
    "            i=i+1\n",
    "        except Exception as error:\n",
    "            print('Problem de dump')\n",
    "            break\n",
    "        \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FOR CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_data(dt):\n",
    "    data = dt['df']\n",
    "    nb_group = dt['nombre_groupe']\n",
    "    km = KModes(n_clusters=nb_group, init='Huang', n_init=5)\n",
    "    km.fit(data.drop('id_groupe',axis=1)) # ICI ICI ICI ICI\n",
    "    tmp_df = pd.DataFrame(pd.Series(km.labels_, name='Cluster'))\n",
    "    data_clustered_encoded=pd.get_dummies(tmp_df,columns=['Cluster']).astype(int)\n",
    "    data_clustered_encoded['id_groupe'] = data['id_groupe']\n",
    "    count_cluster = pd.DataFrame(tmp_df['Cluster'].value_counts()).T\n",
    "    return data_clustered_encoded , count_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "chemin = '/Users/soufiane/Documents/GitHub/universe/My_Universe/Data'\n",
    "fichiers = os.listdir(chemin)\n",
    "Datas = []\n",
    "for nom_fichier in fichiers:\n",
    "    with open(chemin+'/'+nom_fichier, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        data = reshape_dictionnaire_data(data)\n",
    "        Datas.append((data,nom_fichier))\n",
    "Datas = ordonner(Datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataSimuRl_c_15_f_12_g_50\n",
      "Done now we dump\n",
      "1\n",
      "DataSimuRl_c_15_f_5_g_30\n",
      "Done now we dump\n",
      "2\n",
      "DataSimuRl_c_15_f_12_g_30\n",
      "Done now we dump\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "chemin = '/Users/soufiane/Documents/GitHub/universe/My_universe/Data_with_clustering/'\n",
    "i=1\n",
    "for d in Datas[15:]:\n",
    "    print(d[1])\n",
    "    data_clustered_encoded , count_cluster = cluster_data(d[0])\n",
    "    d[0]['data_clustered_encoded'] = data_clustered_encoded\n",
    "    d[0]['count_cluster'] = count_cluster\n",
    "    # On enregistre les données dans le répertoire\n",
    "    print('Done now we dump')\n",
    "    fichier_name = chemin + d[1]\n",
    "    with open(fichier_name, 'wb') as fichier:\n",
    "        try:\n",
    "            pickle.dump(d[0], fichier)\n",
    "            print(i)\n",
    "            i=i+1\n",
    "        except Exception as error:\n",
    "            print('Problem de dump')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
